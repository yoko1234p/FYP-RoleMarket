"""
Enhanced Trends Pipeline - Complete trend extraction and optimization workflow

Integrations:
    1. CategoryTrendsExtractor (social media seed keywords - pytrends)
    2. CategoryTrendsExtractorTrendsPy (trendspy backend)
    3. CategoryTrendsExtractorTrendspyg (trendspyg RSS - ultra-fast)
    4. CategoryTrendsExtractorTrendspygCSV (trendspyg CSV - comprehensive)
    5. KeywordOptimizer (keyword filtering and optimization)
    6. PromptGenerator (LLM prompt generation)

Author: Product Manager (John), Developer (James)
Date: 2025-11-10
Version: 4.0 - trendspyg CSV Integration

Workflow:
    Real-time trends extraction â†’ Keyword optimization â†’ Prompt generation â†’ Image generation
"""

import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from obj1_nlp_prompt.category_trends_extractor import CategoryTrendsExtractor
from obj1_nlp_prompt.category_trends_extractor_trendspy import CategoryTrendsExtractorTrendsPy
from obj1_nlp_prompt.category_trends_extractor_trendspyg import CategoryTrendsExtractorTrendspyg
from obj1_nlp_prompt.category_trends_extractor_trendspyg_csv import CategoryTrendsExtractorTrendspygCSV
from obj1_nlp_prompt.keyword_optimizer import KeywordOptimizer
from obj1_nlp_prompt.prompt_generator import PromptGenerator
import pandas as pd
from typing import Dict, List, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedTrendsPipeline:
    """
    Complete trends extraction and prompt generation pipeline.

    Features:
        - Real-time trending searches (trendspyg RSS - ultra-fast)
        - Comprehensive trends data (trendspyg CSV - advanced filtering)
        - Social media seed keyword auto-discovery
        - Keyword filtering and optimization
        - LLM-based prompt generation
        - Support for 4 backends: trendspyg, trendspyg_csv, trendspy, pytrends

    Backend Comparison:
        - trendspyg (RSS): Ultra-fast (0.2-0.5s), 10-20 trends, no filtering
        - trendspyg_csv (CSV): Slower (~10s), 480 trends, category/time filtering
        - trendspy: Better rate limiting than pytrends
        - pytrends: Original backend (archived, may have issues)
    """

    def __init__(
        self,
        region: str = 'HK',
        lang: str = 'zh-TW',
        backend: str = 'trendspyg',
        proxy: Optional[str] = None,
        request_delay: float = 3.0,
        # trendspyg RSS specific options
        include_images: bool = True,
        include_articles: bool = True,
        max_articles_per_trend: int = 5,
        # trendspyg CSV specific options
        category: str = 'all',
        hours: int = 24,
        active_only: bool = False,
        sort_by: str = 'relevance',
        headless: bool = True
    ):
        """
        Initialize enhanced trends pipeline.

        Args:
            region: Google Trends region code (default: 'HK')
            lang: Language code (default: 'zh-TW')
            backend: Trends backend
                - 'trendspyg': RSS mode - ultra-fast, real-time, 10-20 trends (RECOMMENDED for speed)
                - 'trendspyg_csv': CSV mode - comprehensive, 480 trends, filtering (RECOMMENDED for data)
                - 'trendspy': Better rate limiting than pytrends
                - 'pytrends': Original backend (archived, may have issues)
            proxy: Optional proxy URL (only for trendspy)
            request_delay: Delay between requests in seconds (only for trendspy)

            # RSS Mode Options (backend='trendspyg'):
            include_images: Include images in trends (default: True)
            include_articles: Include news articles (default: True)
            max_articles_per_trend: Max news articles per trend (default: 5)

            # CSV Mode Options (backend='trendspyg_csv'):
            category: Filter by category (default: 'all')
                     Options: 'all', 'autos', 'beauty', 'business', 'climate',
                             'entertainment', 'food', 'games', 'health', 'hobbies',
                             'jobs', 'law', 'other', 'pets', 'politics', 'science',
                             'shopping', 'sports', 'technology', 'travel'
            hours: Time period in hours (default: 24)
                  Options: 4, 24, 48, 168 (7 days)
            active_only: Only return rising/active trends (default: False)
            sort_by: Sort order (default: 'relevance')
                    Options: 'relevance', 'title', 'volume', 'recency'
            headless: Run browser in headless mode (default: True)
        """
        self.backend = backend

        # Initialize trends extractor based on backend
        if backend == 'trendspyg':
            self.trends_extractor = CategoryTrendsExtractorTrendspyg(
                region=region,
                lang=lang,
                include_images=include_images,
                include_articles=include_articles,
                max_articles_per_trend=max_articles_per_trend
            )
            logger.info("EnhancedTrendsPipeline initialized with trendspyg RSS backend (ultra-fast)")
        elif backend == 'trendspyg_csv':
            self.trends_extractor = CategoryTrendsExtractorTrendspygCSV(
                region=region,
                category=category,
                hours=hours,
                active_only=active_only,
                sort_by=sort_by,
                headless=headless
            )
            logger.info(f"EnhancedTrendsPipeline initialized with trendspyg CSV backend")
            logger.info(f"  Category: {category}, Hours: {hours}, Active Only: {active_only}")
        elif backend == 'trendspy':
            self.trends_extractor = CategoryTrendsExtractorTrendsPy(
                region=region,
                lang=lang,
                proxy=proxy,
                request_delay=request_delay
            )
            logger.info(f"EnhancedTrendsPipeline initialized with trendspy backend (delay: {request_delay}s)")
        else:
            self.trends_extractor = CategoryTrendsExtractor(region=region, lang=lang)
            logger.info("EnhancedTrendsPipeline initialized with pytrends backend")

        self.keyword_optimizer = KeywordOptimizer()

        # Use absolute paths for PromptGenerator
        base_dir = Path(__file__).parent.parent
        self.prompt_generator = PromptGenerator(
            template_path=str(base_dir / 'obj1_nlp_prompt' / 'templates' / 'prompt_template.txt'),
            character_desc_path=str(base_dir / 'data' / 'character_descriptions' / 'lulu_pig.txt')
        )

    def extract_and_optimize_trends(
        self,
        timeframe: str,
        theme_filter: str = None,
        categories: List[str] = ['arts_entertainment', 'shopping', 'toys'],
        top_n_raw: int = 50,
        top_n_optimized: int = 15,
        min_visual_score: float = 2.0,
        output_dir: str = 'data/trends_seasonal'
    ) -> Dict:
        """
        æå–ä¸¦å„ªåŒ–è¶¨å‹¢é—œéµå­—ã€‚

        Args:
            timeframe: æ™‚é–“ç¯„åœ (e.g., '2024-11-01 2024-12-31')
            theme_filter: ä¸»é¡Œéæ¿¾ (e.g., 'christmas|è–èª•')
            categories: Google Trends categories
            top_n_raw: åŸå§‹æå–é—œéµå­—æ•¸é‡
            top_n_optimized: å„ªåŒ–å¾Œä¿ç•™æ•¸é‡
            min_visual_score: æœ€ä½è¦–è¦ºåŒ–åˆ†æ•¸
            output_dir: è¼¸å‡ºç›®éŒ„

        Returns:
            Dictionary with:
                - raw_trends: åŸå§‹è¶¨å‹¢ DataFrame
                - optimized_trends: å„ªåŒ–å¾Œè¶¨å‹¢ DataFrame
                - formatted_keywords: æ ¼å¼åŒ–é—œéµå­—ï¼ˆæº–å‚™æäº¤ GPTï¼‰
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"Enhanced Trends Pipeline - æå–èˆ‡å„ªåŒ–")
        logger.info(f"{'='*80}\n")

        # Step 1: æå–ç¤¾äº¤åª’é«”è¶¨å‹¢
        logger.info("Step 1: æå–ç¤¾äº¤åª’é«”è¶¨å‹¢...")
        raw_trends = self.trends_extractor.extract_category_trends(
            timeframe=timeframe,
            categories=categories,
            theme_filter=theme_filter,
            top_n=top_n_raw
        )

        if raw_trends.empty:
            logger.warning("âŒ æœªæå–åˆ°ä»»ä½•è¶¨å‹¢é—œéµå­—ï¼")
            return {
                'raw_trends': raw_trends,
                'optimized_trends': pd.DataFrame(),
                'formatted_keywords': {'keywords': [], 'concepts': [], 'summary': {}}
            }

        # Save raw trends
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        raw_path = f"{output_dir}/raw_trends_{timeframe.replace(' ', '_').replace('-', '')}.csv"
        raw_trends.to_csv(raw_path, index=False)
        logger.info(f"ğŸ’¾ Raw trends saved: {raw_path}\n")

        # Step 2: å„ªåŒ–é—œéµå­—
        logger.info("Step 2: å„ªåŒ–é—œéµå­—...")
        optimized_trends = self.keyword_optimizer.optimize_keywords(
            raw_trends,
            top_n=top_n_optimized,
            min_visual_score=min_visual_score
        )

        # Save optimized trends
        opt_path = f"{output_dir}/optimized_trends_{timeframe.replace(' ', '_').replace('-', '')}.csv"
        optimized_trends.to_csv(opt_path, index=False)
        logger.info(f"ğŸ’¾ Optimized trends saved: {opt_path}\n")

        # Step 3: æ ¼å¼åŒ–ç‚º GPT è¼¸å…¥
        logger.info("Step 3: æ ¼å¼åŒ–é—œéµå­—...")
        formatted_keywords = self.keyword_optimizer.format_for_prompt(optimized_trends)

        logger.info(f"\n{'='*80}")
        logger.info(f"Pipeline å®Œæˆ")
        logger.info(f"{'='*80}")
        logger.info(f"åŸå§‹é—œéµå­—: {len(raw_trends)}")
        logger.info(f"å„ªåŒ–å¾Œé—œéµå­—: {len(optimized_trends)}")
        logger.info(f"æº–å‚™æäº¤ GPT: {len(formatted_keywords['keywords'])} keywords")
        logger.info(f"{'='*80}\n")

        return {
            'raw_trends': raw_trends,
            'optimized_trends': optimized_trends,
            'formatted_keywords': formatted_keywords
        }

    def generate_prompts_from_trends(
        self,
        formatted_keywords: Dict,
        theme: str,
        n_variations: int = 4,
        output_dir: str = 'data/prompts_enhanced'
    ) -> List[Dict]:
        """
        ä½¿ç”¨å„ªåŒ–å¾Œçš„é—œéµå­—ç”Ÿæˆ Promptsã€‚

        Args:
            formatted_keywords: æ ¼å¼åŒ–çš„é—œéµå­—ï¼ˆå¾ extract_and_optimize_trends ç²å¾—ï¼‰
            theme: ä¸»é¡Œåç¨± (e.g., 'Christmas')
            n_variations: Prompt è®Šé«”æ•¸é‡
            output_dir: è¼¸å‡ºç›®éŒ„

        Returns:
            List of prompt dictionaries
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"Prompt ç”Ÿæˆ")
        logger.info(f"{'='*80}\n")

        keywords = formatted_keywords['keywords']
        concepts = formatted_keywords['concepts']

        logger.info(f"Theme: {theme}")
        logger.info(f"Keywords ({len(keywords)}): {', '.join(keywords)}")
        logger.info(f"Visual Concepts: {', '.join(concepts[:5])}...")

        # Generate prompts
        prompts = self.prompt_generator.generate_variations(
            theme=theme,
            keywords=keywords,
            n=n_variations
        )

        # Save prompts
        self.prompt_generator.save_prompts(prompts, theme, output_dir=output_dir)

        logger.info(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(prompts)} å€‹ prompts")
        logger.info(f"ğŸ’¾ Prompts saved to: {output_dir}")
        logger.info(f"{'='*80}\n")

        return prompts

    def run_full_pipeline(
        self,
        theme: str,
        timeframe: str,
        theme_filter: str = None,
        n_prompts: int = 4,
        output_dir_trends: str = 'data/trends_seasonal',
        output_dir_prompts: str = 'data/prompts_enhanced'
    ) -> Dict:
        """
        åŸ·è¡Œå®Œæ•´ Pipelineï¼šè¶¨å‹¢æå– â†’ å„ªåŒ– â†’ Prompt ç”Ÿæˆã€‚

        Args:
            theme: ä¸»é¡Œåç¨± (e.g., 'Christmas')
            timeframe: æ™‚é–“ç¯„åœ (e.g., '2024-11-01 2024-12-31')
            theme_filter: ä¸»é¡Œéæ¿¾ regex (optional)
            n_prompts: ç”Ÿæˆ Prompt æ•¸é‡
            output_dir_trends: è¶¨å‹¢è¼¸å‡ºç›®éŒ„
            output_dir_prompts: Prompts è¼¸å‡ºç›®éŒ„

        Returns:
            Dictionary with all pipeline results
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"Enhanced Trends Pipeline - Full Execution")
        logger.info(f"{'='*80}")
        logger.info(f"Theme: {theme}")
        logger.info(f"Timeframe: {timeframe}")
        logger.info(f"Theme Filter: {theme_filter or 'None'}")
        logger.info(f"{'='*80}\n")

        # Step 1+2: Extract and optimize trends
        trends_result = self.extract_and_optimize_trends(
            timeframe=timeframe,
            theme_filter=theme_filter,
            output_dir=output_dir_trends
        )

        if trends_result['optimized_trends'].empty:
            logger.error("âŒ Pipeline å¤±æ•—ï¼šæœªç²å¾—å„ªåŒ–å¾Œé—œéµå­—")
            return trends_result

        # Step 3: Generate prompts
        prompts = self.generate_prompts_from_trends(
            formatted_keywords=trends_result['formatted_keywords'],
            theme=theme,
            n_variations=n_prompts,
            output_dir=output_dir_prompts
        )

        # Complete result
        result = {
            **trends_result,
            'prompts': prompts,
            'pipeline_summary': {
                'theme': theme,
                'timeframe': timeframe,
                'raw_keywords_count': len(trends_result['raw_trends']),
                'optimized_keywords_count': len(trends_result['optimized_trends']),
                'prompts_generated': len(prompts)
            }
        }

        logger.info(f"\n{'='*80}")
        logger.info(f"Pipeline åŸ·è¡Œå®Œæˆ")
        logger.info(f"{'='*80}")
        logger.info(f"âœ… åŸå§‹é—œéµå­—: {result['pipeline_summary']['raw_keywords_count']}")
        logger.info(f"âœ… å„ªåŒ–é—œéµå­—: {result['pipeline_summary']['optimized_keywords_count']}")
        logger.info(f"âœ… ç”Ÿæˆ Prompts: {result['pipeline_summary']['prompts_generated']}")
        logger.info(f"{'='*80}\n")

        return result


def main():
    """
    æ¸¬è©¦å®Œæ•´ Enhanced Trends Pipeline.
    """
    # Initialize pipeline
    pipeline = EnhancedTrendsPipeline()

    # Run full pipeline for Christmas
    result = pipeline.run_full_pipeline(
        theme='Christmas',
        timeframe='2024-11-01 2024-12-31',
        theme_filter=None,  # ä¸éæ¿¾ï¼Œä½¿ç”¨æ‰€æœ‰ç¤¾äº¤åª’é«”è¶¨å‹¢
        n_prompts=4,
        output_dir_trends='data/trends_seasonal',
        output_dir_prompts='data/prompts_enhanced'
    )

    # Display results summary
    print("\n" + "="*80)
    print("Pipeline åŸ·è¡Œç¸½çµ")
    print("="*80)
    print(f"ä¸»é¡Œ: {result['pipeline_summary']['theme']}")
    print(f"æ™‚æ®µ: {result['pipeline_summary']['timeframe']}")
    print(f"åŸå§‹é—œéµå­—: {result['pipeline_summary']['raw_keywords_count']}")
    print(f"å„ªåŒ–é—œéµå­—: {result['pipeline_summary']['optimized_keywords_count']}")
    print(f"ç”Ÿæˆ Prompts: {result['pipeline_summary']['prompts_generated']}")
    print("="*80)

    # Display prompts
    print("\nç”Ÿæˆçš„ Prompts:")
    for i, prompt_data in enumerate(result['prompts'], 1):
        print(f"\nVariation {i}:")
        print(f"  Length: {prompt_data['length']} words")
        print(f"  Prompt: {prompt_data['prompt'][:150]}...")


if __name__ == '__main__':
    main()
