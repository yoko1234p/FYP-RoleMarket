"""
é—œéµå­—å„ªåŒ–å™¨ - æäº¤ GPT å‰éæ¿¾å’Œå„ªåŒ–é—œéµå­—

Author: Product Manager (John)
Date: 2025-10-27
Version: 1.0 - Enhancement v1.2

Purpose:
    å¾ç¤¾äº¤åª’é«”è¶¨å‹¢æå–çš„é—œéµå­—ä¸­ï¼Œéæ¿¾å‡ºé©åˆ Character IP è¨­è¨ˆçš„è¦–è¦ºåŒ–é—œéµå­—ã€‚

Filters:
    1. ç§»é™¤æŠ€è¡“æ€§/å¹³å°åç¨±ï¼ˆinstagram, facebook, downloadï¼‰
    2. ç§»é™¤éæ–¼å…·é«”çš„é›»å½±/æ˜æ˜Ÿåç¨±
    3. å„ªå…ˆé¸æ“‡è¦–è¦ºåŒ–ã€æƒ…æ„ŸåŒ–çš„é—œéµå­—
    4. æå–å¯è¨­è¨ˆçš„å…ƒç´ ï¼ˆmeme é¢¨æ ¼ã€ç¯€æ—¥å…ƒç´ ã€æƒ…æ„Ÿè¡¨é”ï¼‰
"""

import pandas as pd
from typing import List, Dict, Set
import re
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KeywordOptimizer:
    """
    å„ªåŒ–å’Œéæ¿¾é—œéµå­—ï¼Œç¢ºä¿é©åˆ Character IP è¨­è¨ˆã€‚
    """

    # é»‘åå–®ï¼šæŠ€è¡“æ€§/å¹³å°è©å½™
    BLACKLIST_PATTERNS = [
        # å¹³å°åç¨±
        r'instagram', r'facebook', r'tiktok', r'å°ç´…æ›¸',
        # æŠ€è¡“æ“ä½œ
        r'download', r'login', r'ç·šä¸Šçœ‹', r'app',
        # éæ–¼å…·é«”çš„åœ°é»
        r'ç¥¨æˆ¿', r'ç´€éŒ„', r'ä¸Šæ˜ ',
        # æœå°‹è¡Œç‚º
        r'çœ‹', r'æŸ¥', r'æœå°‹'
    ]

    # è¦–è¦ºåŒ–é—œéµå­—ç‰¹å¾µ
    VISUAL_INDICATORS = [
        'meme', 'cute', 'kawaii', 'å¯æ„›',
        'style', 'é¢¨æ ¼', 'character', 'è§’è‰²',
        'design', 'è¨­è¨ˆ', 'art', 'è—è¡“',
        'cozy', 'æº«é¦¨', 'festive', 'ç¯€æ—¥',
        'decoration', 'è£é£¾', 'celebration', 'æ…¶ç¥'
    ]

    # æƒ…æ„Ÿ/æ°›åœé—œéµå­—ï¼ˆé«˜åƒ¹å€¼ï¼‰
    EMOTIONAL_KEYWORDS = [
        'chill', 'happy', 'cozy', 'warm', 'festive',
        'cheerful', 'cute', 'adorable', 'lovely',
        'æº«é¦¨', 'å¯æ„›', 'æ­¡æ¨‚', 'æº«æš–', 'ç¯€æ—¥'
    ]

    # ç¯€æ—¥/ä¸»é¡Œé—œéµå­—ï¼ˆé«˜åƒ¹å€¼ï¼‰
    THEME_KEYWORDS = [
        'christmas', 'è–èª•', 'xmas', 'santa',
        'halloween', 'è¬è–ç¯€', 'pumpkin',
        'spring', 'æ˜¥å¤©', 'lunar new year',
        'valentine', 'æƒ…äººç¯€'
    ]

    def __init__(self):
        """Initialize keyword optimizer."""
        logger.info("KeywordOptimizer initialized")

    def is_blacklisted(self, keyword: str) -> bool:
        """
        æª¢æŸ¥é—œéµå­—æ˜¯å¦åœ¨é»‘åå–®ä¸­ã€‚

        Args:
            keyword: é—œéµå­—

        Returns:
            True if blacklisted, False otherwise
        """
        keyword_lower = keyword.lower()
        for pattern in self.BLACKLIST_PATTERNS:
            if re.search(pattern, keyword_lower):
                return True
        return False

    def calculate_visual_score(self, keyword: str) -> float:
        """
        è¨ˆç®—é—œéµå­—çš„è¦–è¦ºåŒ–åˆ†æ•¸ã€‚

        åˆ†æ•¸è¶Šé«˜ = è¶Šé©åˆè¦–è¦ºè¨­è¨ˆ

        Scoring:
            - åŒ…å«è¦–è¦ºåŒ–æŒ‡æ¨™è©: +2.0
            - åŒ…å«æƒ…æ„Ÿ/æ°›åœè©: +3.0 (é«˜åƒ¹å€¼)
            - åŒ…å«ç¯€æ—¥/ä¸»é¡Œè©: +4.0 (æœ€é«˜åƒ¹å€¼)
            - åŒ…å« "meme": +1.5 (æµè¡Œæ–‡åŒ–)
            - é•·åº¦é©ä¸­ (2-4 words): +0.5
            - åŒ…å«ä¸­æ–‡: +0.3 (æœ¬åœ°åŒ–)

        Args:
            keyword: é—œéµå­—

        Returns:
            è¦–è¦ºåŒ–åˆ†æ•¸ (0-10)
        """
        keyword_lower = keyword.lower()
        score = 0.0

        # ç¯€æ—¥/ä¸»é¡Œé—œéµå­—ï¼ˆæœ€é«˜åƒ¹å€¼ï¼‰
        for theme_word in self.THEME_KEYWORDS:
            if theme_word in keyword_lower:
                score += 4.0
                break

        # æƒ…æ„Ÿ/æ°›åœé—œéµå­—ï¼ˆé«˜åƒ¹å€¼ï¼‰
        for emotion_word in self.EMOTIONAL_KEYWORDS:
            if emotion_word in keyword_lower:
                score += 3.0
                break

        # è¦–è¦ºåŒ–æŒ‡æ¨™
        for visual_word in self.VISUAL_INDICATORS:
            if visual_word in keyword_lower:
                score += 2.0
                break

        # Meme æ–‡åŒ–ï¼ˆæµè¡Œï¼‰
        if 'meme' in keyword_lower:
            score += 1.5

        # é•·åº¦é©ä¸­
        word_count = len(keyword.split())
        if 2 <= word_count <= 4:
            score += 0.5

        # æœ¬åœ°åŒ–ï¼ˆåŒ…å«ä¸­æ–‡ï¼‰
        if re.search(r'[\u4e00-\u9fff]', keyword):
            score += 0.3

        return min(score, 10.0)  # Cap at 10

    def extract_visual_concepts(self, keyword: str) -> List[str]:
        """
        å¾é—œéµå­—ä¸­æå–è¦–è¦ºåŒ–æ¦‚å¿µã€‚

        Examples:
            "christmas meme" â†’ ["christmas", "meme style"]
            "chill guy" â†’ ["chill", "relaxed mood"]
            "è–èª•é›»å½±" â†’ ["christmas", "cinematic style"]

        Args:
            keyword: é—œéµå­—

        Returns:
            è¦–è¦ºåŒ–æ¦‚å¿µåˆ—è¡¨
        """
        concepts = []
        keyword_lower = keyword.lower()

        # ç¯€æ—¥æ¦‚å¿µ
        if any(x in keyword_lower for x in ['christmas', 'è–èª•', 'xmas']):
            concepts.append('christmas theme')
        if any(x in keyword_lower for x in ['halloween', 'è¬è–ç¯€']):
            concepts.append('halloween theme')

        # é¢¨æ ¼æ¦‚å¿µ
        if 'meme' in keyword_lower:
            concepts.append('meme style')
        if any(x in keyword_lower for x in ['é›»å½±', 'movie', 'film', 'cinema']):
            concepts.append('cinematic style')
        if any(x in keyword_lower for x in ['cute', 'å¯æ„›', 'kawaii']):
            concepts.append('cute aesthetic')

        # æƒ…ç·’æ¦‚å¿µ
        if 'chill' in keyword_lower:
            concepts.append('relaxed mood')
        if 'happy' in keyword_lower:
            concepts.append('cheerful mood')
        if any(x in keyword_lower for x in ['cozy', 'æº«é¦¨']):
            concepts.append('cozy atmosphere')

        return concepts

    def optimize_keywords(
        self,
        keywords_df: pd.DataFrame,
        top_n: int = 15,
        min_visual_score: float = 2.0,
        preserve_high_trend: bool = True
    ) -> pd.DataFrame:
        """
        å„ªåŒ–é—œéµå­—åˆ—è¡¨ã€‚

        Workflow:
            1. ç§»é™¤é»‘åå–®é—œéµå­—
            2. è¨ˆç®—è¦–è¦ºåŒ–åˆ†æ•¸
            3. æå–è¦–è¦ºåŒ–æ¦‚å¿µ
            4. æŒ‰åˆ†æ•¸æ’åº
            5. è¿”å› Top N

        Args:
            keywords_df: é—œéµå­— DataFrame (must have 'keyword', 'trend_score')
            top_n: è¿”å›å‰ N å€‹é—œéµå­—
            min_visual_score: æœ€ä½è¦–è¦ºåŒ–åˆ†æ•¸é–¾å€¼
            preserve_high_trend: ä¿ç•™é«˜è¶¨å‹¢åˆ†æ•¸é—œéµå­—ï¼ˆå³ä½¿è¦–è¦ºåˆ†æ•¸ä½ï¼‰

        Returns:
            å„ªåŒ–å¾Œçš„ DataFrame with additional columns:
                - visual_score: è¦–è¦ºåŒ–åˆ†æ•¸
                - visual_concepts: è¦–è¦ºåŒ–æ¦‚å¿µ
                - combined_score: ç¶œåˆåˆ†æ•¸
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"é—œéµå­—å„ªåŒ–æµç¨‹")
        logger.info(f"{'='*60}\n")
        logger.info(f"Input: {len(keywords_df)} keywords")

        # Step 1: ç§»é™¤é»‘åå–®
        df = keywords_df.copy()
        df['is_blacklisted'] = df['keyword'].apply(self.is_blacklisted)
        df_filtered = df[~df['is_blacklisted']].copy()

        logger.info(f"Step 1: ç§»é™¤é»‘åå–® â†’ {len(df_filtered)} keywords remaining")

        # Step 2: è¨ˆç®—è¦–è¦ºåŒ–åˆ†æ•¸
        df_filtered['visual_score'] = df_filtered['keyword'].apply(
            self.calculate_visual_score
        )

        # Step 3: æå–è¦–è¦ºåŒ–æ¦‚å¿µ
        df_filtered['visual_concepts'] = df_filtered['keyword'].apply(
            self.extract_visual_concepts
        )

        # Step 4: è¨ˆç®—ç¶œåˆåˆ†æ•¸
        # Normalize trend_score to 0-10
        max_trend = df_filtered['trend_score'].max()
        if max_trend > 0:
            df_filtered['normalized_trend'] = (
                df_filtered['trend_score'] / max_trend * 10
            )
        else:
            df_filtered['normalized_trend'] = 0

        # Combined score: 70% visual + 30% trend
        df_filtered['combined_score'] = (
            0.7 * df_filtered['visual_score'] +
            0.3 * df_filtered['normalized_trend']
        )

        logger.info(f"Step 2-4: è¨ˆç®—åˆ†æ•¸å®Œæˆ")

        # Step 5: éæ¿¾ä½åˆ†é—œéµå­—
        df_filtered = df_filtered[
            df_filtered['visual_score'] >= min_visual_score
        ]

        logger.info(f"Step 5: éæ¿¾ä½åˆ† (>={min_visual_score}) â†’ {len(df_filtered)} keywords")

        # Step 6: æ’åºä¸¦é¸æ“‡ Top N
        df_sorted = df_filtered.sort_values('combined_score', ascending=False)
        df_top = df_sorted.head(top_n)

        logger.info(f"Step 6: é¸æ“‡ Top {top_n} â†’ {len(df_top)} keywords")
        logger.info(f"\n{'='*60}")
        logger.info(f"å„ªåŒ–å®Œæˆ")
        logger.info(f"{'='*60}\n")

        # é¡¯ç¤º Top 10
        logger.info("Top 10 å„ªåŒ–å¾Œé—œéµå­—:")
        for idx, row in df_top.head(10).iterrows():
            concepts_str = ', '.join(row['visual_concepts']) if row['visual_concepts'] else 'N/A'
            logger.info(
                f"  {idx+1:2d}. {row['keyword']:30s} "
                f"(è¦–è¦º: {row['visual_score']:.1f}, "
                f"è¶¨å‹¢: {row['normalized_trend']:.1f}, "
                f"ç¸½åˆ†: {row['combined_score']:.1f})"
            )
            if row['visual_concepts']:
                logger.info(f"      â†’ Concepts: {concepts_str}")

        return df_top

    def format_for_prompt(
        self,
        optimized_df: pd.DataFrame,
        include_concepts: bool = True
    ) -> Dict[str, any]:
        """
        æ ¼å¼åŒ–å„ªåŒ–å¾Œçš„é—œéµå­—ï¼Œæº–å‚™æäº¤çµ¦ GPTã€‚

        Args:
            optimized_df: å„ªåŒ–å¾Œçš„ DataFrame
            include_concepts: æ˜¯å¦åŒ…å«è¦–è¦ºåŒ–æ¦‚å¿µ

        Returns:
            Dictionary with:
                - keywords: List of keyword strings
                - concepts: List of visual concepts
                - summary: æ‘˜è¦ä¿¡æ¯
        """
        keywords = optimized_df['keyword'].tolist()

        # æå–æ‰€æœ‰è¦–è¦ºåŒ–æ¦‚å¿µï¼ˆå»é‡ï¼‰
        all_concepts = []
        for concepts_list in optimized_df['visual_concepts']:
            all_concepts.extend(concepts_list)
        unique_concepts = list(set(all_concepts))

        summary = {
            'total_keywords': len(keywords),
            'avg_visual_score': optimized_df['visual_score'].mean(),
            'avg_trend_score': optimized_df['normalized_trend'].mean(),
            'top_concept': max(set(all_concepts), key=all_concepts.count) if all_concepts else None
        }

        result = {
            'keywords': keywords,
            'concepts': unique_concepts if include_concepts else [],
            'summary': summary
        }

        logger.info(f"\n{'='*60}")
        logger.info(f"æ ¼å¼åŒ–å®Œæˆ - æº–å‚™æäº¤ GPT")
        logger.info(f"{'='*60}")
        logger.info(f"é—œéµå­—æ•¸é‡: {len(keywords)}")
        logger.info(f"è¦–è¦ºåŒ–æ¦‚å¿µ: {', '.join(unique_concepts[:5])}..." if unique_concepts else "è¦–è¦ºåŒ–æ¦‚å¿µ: N/A")
        logger.info(f"å¹³å‡è¦–è¦ºåˆ†æ•¸: {summary['avg_visual_score']:.2f}")
        logger.info(f"ä¸»è¦æ¦‚å¿µ: {summary['top_concept']}")
        logger.info(f"{'='*60}\n")

        return result


def main():
    """
    æ¸¬è©¦é—œéµå­—å„ªåŒ–å™¨ã€‚
    """
    # Load social media trends
    logger.info("è¼‰å…¥ç¤¾äº¤åª’é«”è¶¨å‹¢æ•¸æ“š...")
    df = pd.read_csv('data/trends_seasonal/nov_dec_social_media_all.csv')

    logger.info(f"åŸå§‹æ•¸æ“š: {len(df)} keywords\n")

    # Initialize optimizer
    optimizer = KeywordOptimizer()

    # Optimize keywords
    optimized = optimizer.optimize_keywords(
        df,
        top_n=15,
        min_visual_score=2.0
    )

    # Save optimized keywords
    optimized.to_csv(
        'data/trends_seasonal/nov_dec_optimized_keywords.csv',
        index=False
    )
    logger.info(f"ğŸ’¾ Saved optimized keywords to: data/trends_seasonal/nov_dec_optimized_keywords.csv")

    # Format for GPT
    formatted = optimizer.format_for_prompt(optimized)

    logger.info("\n" + "="*80)
    logger.info("æº–å‚™æäº¤çµ¦ GPT çš„é—œéµå­—:")
    logger.info("="*80)
    for idx, kw in enumerate(formatted['keywords'], 1):
        logger.info(f"  {idx:2d}. {kw}")

    logger.info("\nè¦–è¦ºåŒ–æ¦‚å¿µ:")
    for concept in formatted['concepts']:
        logger.info(f"  - {concept}")


if __name__ == '__main__':
    main()
